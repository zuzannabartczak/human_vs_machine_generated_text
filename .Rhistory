demo()
# Load the package required to read JSON files.
install.packages("rjson") # Optional
library("rjson")
# Give the input file name to the function.
myData <- fromJSON(file="SemEval2024-Task8-data/SubtaskA/subtaskA_dev_monolingual.jsonl")
# Print the result.
print(myData)
# Load the package required to read JSON files.
install.packages("rjson") # Optional
install.packages("rjson")
library("rjson")
# Give the input file name to the function.
myData <- fromJSON(file="subtaskA_dev_monolingual.jsonl")
# Load the package required to read JSON files.
install.packages("rjson") # Optional
install.packages("rjson")
library("rjson")
# Give the input file name to the function.
myData <- fromJSON(file="subtaskA_dev_monolingual..jsonl")
# Give the input file name to the function.
myData <- fromJSON(file="subtaskA_dev_monolingual.jsonl")
# Give the input file name to the function.
myData <- file="subtaskA_dev_monolingual.csv
# Print the result.
print(myData)
# Give the input file name to the function.
myData <- file="subtaskA_dev_monolingual.csv
data <- read.csv("subtaskA_dev_monolingual.csv")
print(data)
print(mydata)
mydata <- read.csv("subtaskA_dev_monolingual.csv")
setwd("~/language technology/statistics/project/human_vs_machine_generated_text")
mydata <- read.csv("subtaskA_dev_monolingual.csv")
print(mydata)
View(mydata)
library(tidyverse)
# Read CSV data
data <- read.csv("subtaskA_dev_monolingual.csv")
ggplot(data, aes(x = header1, y = header2)) + geom_point() + labs(x = "Header 1", y = "Header 2", title = "My Plot")
ggplot(data, aes(x = header1)) + geom_point() + labs(x = "Header 1", title = "My Plot")
ggplot(mydata, aes(x = header1, y = header2)) +
geom_point() +
labs(x = "Header 1", y = "Header 2", title = "My Plot")
labs(x = "Header 1", title = "My Plot")
View(data)
taskAmono <- read.csv("subtaskA_dev_monolingual.csv", header = TRUE)
taskAmulti <- read.csv("subtaskA_dev_multilingual.csv", header = TRUE)
taskB <- read.csv("subtaskB_dev.csv", header = TRUE)
taskC <- read.csv("subtaskC_train.csv", header = TRUE)
library(ggplot2)
# Distribution of human vs. machine-generated texts
ggplot(taskAmono, aes(x = label)) +
geom_bar() +
ggtitle("Distribution of Human vs. Machine-Generated Texts in Monolingual Dataset")
ggplot(taskAmulti, aes(x = label)) +
geom_bar() +
ggtitle("Distribution of Human vs. Machine-Generated Texts in Multilingual Dataset")
# Distribution across different languages and domains
ggplot(taskB, aes(x = model, fill = label)) +
geom_bar(position = "dodge") +
ggtitle("Distribution of Human vs. Machine-Generated Texts Across Languages in Subtask B")
ggplot(taskB, aes(x = source, fill = label)) +
geom_bar(position = "dodge") +
ggtitle("Distribution of Human vs. Machine-Generated Texts Across Domains in Subtask C")
View(taskC)
View(taskB)
View(taskAmono)
ssss <- fromJSON(subtaskB_train.json)
library(jsonlite)
ssss <- fromJSON(subtaskB_train.json)
ssss <- fromJSON(subtaskB_train.jsonl)
install.packages("rjson")
install.packages("rjson")
library("rjson")
ssss <- fromJSON("subtaskB_train.jsonl")
ssss <- fromJSON("subtaskB_train.jsonl", flatten = TRUE)
ssss <- fromJSON("subtaskB_train.jsonl")
ssss <- fromJSON(file = "subtaskB_train.jsonl")
ssss <- fromJSON(file = "subtaskB_train.jsonl")
ssss
print(ssss)
ssss <- fromJSON(file = "subtaskB_train.jsonl")
json_data_frame <- as.data.frame(sss)
ssss <- fromJSON(file = "subtaskB_train.jsonl")
ssss <- rjson::fromJSON(file = "subtaskB_train.jsonl")
library(ggplot2)
taskAmono <- read.csv("data_csv/subtaskA_dev_monolingual.csv", header = TRUE)
taskAmulti <- read.csv("data_csv/subtaskA_dev_multilingual.csv", header = TRUE)
taskB <- read.csv("data_csv/subtaskB_dev.csv", header = TRUE)
taskC <- read.csv("data_csv/subtaskC_train.csv", header = TRUE)
# Load the learning curve data
learning_curve <- read.csv("learning_curve.csv")
# Plot the learning curve
ggplot(learning_curve, aes(x = train_sizes)) +
geom_line(aes(y = train_scores_mean), color = "blue", size = 1) +
geom_line(aes(y = test_scores_mean), color = "red", size = 1) +
geom_ribbon(aes(ymin = train_scores_mean - train_scores_std / 2,
ymax = train_scores_mean + train_scores_std / 2),
fill = "blue", alpha = 0.2) +
geom_ribbon(aes(ymin = test_scores_mean - test_scores_std / 2,
ymax = test_scores_mean + test_scores_std / 2),
fill = "red", alpha = 0.2) +
labs(x = "Training examples", y = "Score", color = "Legend") +
scale_color_manual(values = c("blue", "red"), labels = c("Training score", "Cross-validation score")) +
theme_minimal()
# Define the confusion matrix
confusion_matrix <- matrix(c[[12198   298]
# Define the confusion matrix
confusion_matrix <- matrix(c(12198, 298, 7590, 3866)), nrow = 2, byrow = TRUE)
# Define the confusion matrix
confusion_matrix <- matrix(c(50, 10, 5, 35), nrow = 2, byrow = TRUE)
# Define the confusion matrix
confusion_matrix <- matrix(c(12198, 10, 5, 35), nrow = 2, byrow = TRUE)
# Define the confusion matrix
confusion_matrix <- matrix(c(12198, 298, 7590, 35), nrow = 2, byrow = TRUE)
# Define the confusion matrix
confusion_matrix <- matrix(c(12198, 298, 7590, 3866), nrow = 2, byrow = TRUE)
# Convert the confusion matrix to a data frame
confusion_matrix_df <- data.frame(
actual = rep(c("Positive", "Negative"), each = 2),
predicted = rep(c("Positive", "Negative"), times = 2),
count = as.vector(confusion_matrix)
)
# Plot the confusion matrix
ggplot(confusion_matrix_df, aes(x = actual, y = predicted, fill = count)) +
geom_tile() +
geom_text(aes(label = count)) +
scale_fill_gradient(low = "white", high = "blue") +
labs(x = "Actual", y = "Predicted", fill = "Count") +
theme_minimal()
View(confusion_matrix)
library(ggplot2)
library(dplyr)
library(nortest)
#Load data
train_data <- read.csv("data_csv/subtaskA_train_monolingual.csv", header = TRUE)
dev_data <- read.csv("data_csv/subtaskA_dev_monolingual.csv", header = TRUE)
# Distribution of human vs. machine-generated texts
ggplot(train_data, aes(x = model, fill = label)) +
geom_bar(position = "dodge") +
ggtitle("Distribution of Human vs. Machine-Generated Texts")
ggplot(train_data, aes(x = model, fill = source)) +
geom_bar(position = "dodge") +
ggtitle("Distribution of domains")
# Extract the 'label' column from the data
labels <- train_data$label
# Convert the data to a numeric format
labels <- as.numeric(labels)
# Test for normality using the Anderson-Darling test
result <- ad.test(labels)
View(result)
# Test for normality using the Anderson-Darling test
result <- ad.test(labels)
# Print the test statistic and p-value
cat("Anderson-Darling test statistic:", result$statistic, "\n")
cat("p-value:", result$p.value, "\n")
# Interpret the results
if (result$p.value > 0.05) {
cat("The data is normally distributed.\n")
} else {
cat("The data is not normally distributed.\n")
}
# Load the data for the RNN model
rnn_data <- ...
first <- read.csv("SGD_outputs/random_training.csv", header = TRUE)
# Print the mean and standard deviation for the RNN model
cat("RNN model:\n")
cat("Mean:", rnn_mean, "\n")
cat("Standard deviation:", rnn_std, "\n")
first <- read.csv("SGD_outputs/random_training.csv", header = TRUE)
# Create a data frame with the weights
weights_df <- read.csv("weights.csv", header = TRUE)
library(ggplot2)
library(dplyr)
library(nortest)
library(jsonlite)
train_data <- read.csv("data_csv/subtaskA_train_monolingual.csv", header = TRUE)
dev_data <- read.csv("data_csv/subtaskA_dev_monolingual.csv", header = TRUE)
sgd_lc <- read.csv("SGD_outputs/learning_curve.csv", header = TRUE)
sgd_first <- read.csv("SGD_outputs/first_training.csv", header = TRUE)
View(sgd_first)
sgd_weights <- read.csv("SGD_outputs/weights.csv", header = TRUE)
rnn_weights <- read.csv("RNN_outputs/weights.csv", header = TRUE)
lines2 <- readLines("RNN_outputs/dev_predictions.jsonl")
lines2 <- lapply(lines2, fromJSON)
rnn_pred <- bind_rows(lines2)
# Distribution of human vs. machine-generated texts
ggplot(train_data, aes(x = model, fill = label)) +
geom_bar(position = "dodge") +
ggtitle("Distribution of Human vs. Machine-Generated Texts")
ggplot(train_data, aes(x = model, fill = source)) +
geom_bar(position = "dodge") +
ggtitle("Distribution of domains")
# Calculate text length
train_data$length <- str_length(train_data$text)
library(tidyverse)
# Calculate text length
train_data$length <- str_length(train_data$text)
# Plot the text length distribution
ggplot(train_data, aes(x = length)) +
geom_histogram(binwidth = 1) +
labs(title = "Text Length Distribution", x = "Text Length", y = "Frequency")
# Calculate text length
train_data$length <- str_length(train_data$text)
# Plot the text length distribution
ggplot(train_data, aes(x = length)) +
geom_histogram(binwidth = 0.5) +
labs(title = "Text Length Distribution", x = "Text Length", y = "Frequency")
# Plot the text length distribution
ggplot(train_data, aes(x = length)) +
geom_histogram(binwidth = 2) +
labs(title = "Text Length Distribution", x = "Text Length", y = "Frequency")
# Plot weights
ggplot(sgd_weights, aes(x = word, y = weight)) +
geom_bar(stat = 'identity') +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Feature Weights", x = "Feature", y = "Weight")
# Plot the distribution of text length
ggplot(train_data, aes(x = length)) +
geom_histogram(binwidth = 100, fill = "blue", color = "black") +
labs(title = "Distribution of Text Length", x = "Text Length", y = "Frequency")
library(ggplot2)
library(dplyr)
library(nortest)
library(jsonlite)
library(tidyverse)
# Plot the distribution of text length
ggplot(train_data, aes(x = length)) +
geom_histogram(binwidth = 100, fill = "blue", color = "black") +
labs(title = "Distribution of Text Length", x = "Text Length", y = "Frequency")
ggplot(train_data, aes(nchar(text), fill = length)) + geom_density(alpha = 0.5)
# Define the percentiles to remove from each side
lower_percentile <- 0.10  # Remove bottom 10%
upper_percentile <- 0.70  # Remove top 10%
# Define the percentiles to remove from each side
lower_percentile <- 0.10  # Remove bottom 10%
upper_percentile <- 0.70  # Remove top 10%
# Calculate the corresponding text length for each percentile
lower_limit <- quantile(train_data$text_length, lower_percentile)
upper_limit <- quantile(train_data$text_length, upper_percentile)
# Calculate the corresponding text length for each percentile
lower_limit <- quantile(train_data$length, lower_percentile)
upper_limit <- quantile(train_data$length, upper_percentile)
# Filter out texts outside the desired range
length_filtered <- train_data %>% filter(length >= lower_limit & length <= upper_limit)
# Plot the distribution of text length
ggplot(df_filtered, aes(x = text_length)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black") +
labs(title = "Distribution of Text Length", x = "Text Length", y = "Frequency")
# Plot the distribution of text length
ggplot(length_filtered, aes(x = text_length)) +
geom_histogram(binwidth = 50, fill = "blue", color = "black") +
labs(title = "Distribution of Text Length", x = "Text Length", y = "Frequency")
# Plot the distribution of text length
ggplot(length_filtered, aes(x = length)) +
geom_histogram(binwidth = 50, fill = "blue", color = "black") +
labs(title = "Distribution of Text Length", x = "Text Length", y = "Frequency")
# Plot the distribution of text length
ggplot(length_filtered, aes(x = length)) +
geom_histogram(binwidth = 10, fill = "blue", color = "black") +
labs(title = "Distribution of Text Length", x = "Text Length", y = "Frequency")
# Plot the distribution of text length
ggplot(length_filtered, aes(x = length)) +
geom_histogram(binwidth = 5, fill = "blue", color = "black") +
labs(title = "Distribution of Text Length", x = "Text Length", y = "Frequency")
upper_percentile <- 0.80  # Remove top 10%
upper_limit <- quantile(train_data$length, upper_percentile)
# Filter out texts outside the desired range
length_filtered <- train_data %>% filter(length >= lower_limit & length <= upper_limit)
# Plot the distribution of text length
ggplot(length_filtered, aes(x = length)) +
geom_histogram(binwidth = 5, fill = "blue", color = "black") +
labs(title = "Distribution of Text Length", x = "Text Length", y = "Frequency")
# Define the percentiles to remove from each side
lower_percentile <- 0.10  # Remove bottom 10%
upper_percentile <- 0.90  # Remove top 10%
# Calculate the corresponding text length for each percentile
lower_limit <- quantile(train_data$length, lower_percentile)
upper_limit <- quantile(train_data$length, upper_percentile)
# Filter out texts outside the desired range
length_filtered <- train_data %>% filter(length >= lower_limit & length <= upper_limit)
# Plot the distribution of text length
ggplot(length_filtered, aes(x = length)) +
geom_histogram(binwidth = 5, fill = "blue", color = "black") +
labs(title = "Distribution of Text Length", x = "Text Length", y = "Frequency")
# Plot the distribution of text length
ggplot(length_filtered, aes(x = length)) +
geom_histogram(binwidth = 5, fill = "pink", color = "black") +
labs(title = "Distribution of Text Length", x = "Text Length", y = "Frequency")
# Plot the distribution of text length
ggplot(length_filtered, aes(x = length)) +
geom_histogram(binwidth = 5, fill = "pink", color = "pink") +
labs(title = "Distribution of Text Length", x = "Text Length", y = "Frequency")
# Plot the distribution of text length
ggplot(length_filtered, aes(x = length)) +
geom_histogram(binwidth = 5, fill = "pink", color = "lightblue") +
labs(title = "Distribution of Text Length", x = "Text Length", y = "Frequency")
# Define the confusion matrix
cm1 <- matrix(c(12198, 298, 7590, 3866), nrow = 2, byrow = TRUE)
# Convert the confusion matrix to a data frame
cm_sgd <- data.frame(
actual = rep(c("Positive", "Negative"), each = 2),
predicted = rep(c("Positive", "Negative"), times = 2),
count = as.vector(cm1)
)
# Plot the confusion matrix
ggplot(cf_sgd, aes(x = actual, y = predicted, fill = count)) +
geom_tile() +
geom_text(aes(label = count)) +
scale_fill_gradient(low = "white", high = "blue") +
labs(x = "Actual", y = "Predicted", fill = "Count") +
theme_minimal()
# Plot the confusion matrix
ggplot(cm_sgd, aes(x = actual, y = predicted, fill = count)) +
geom_tile() +
geom_text(aes(label = count)) +
scale_fill_gradient(low = "white", high = "blue") +
labs(x = "Actual", y = "Predicted", fill = "Count") +
theme_minimal()
# Plot the confusion matrix
ggplot(as.data.frame(cm_sgd), aes(x = Reference, y = Prediction)) +
geom_tile(aes(fill = log(Freq)), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
labs(fill = "Log(Frequency)")
# Plot the confusion matrix
ggplot(as.data.frame(cm_sgd), aes(x = actual, y = predicted, fill = count)) +
geom_tile(aes(fill = log(Freq)), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
labs(fill = "Log(Frequency)")
# Plot the confusion matrix
ggplot(as.data.frame(cm_sgd), aes(x = actual, y = predicted, fill = count)) +
geom_tile(aes(fill = log(count)), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", count)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
labs(fill = "Log(Frequency)")
# Plot the confusion matrix
ggplot(cm_sgd, aes(x = actual, y = predicted, fill = count)) +
geom_tile() +
geom_text(aes(label = count)) +
scale_fill_gradient(low = "white", high = "blue") +
labs(x = "Actual", y = "Predicted", fill = "Count") +
theme_minimal()
# Plot the confusion matrix
ggplot(cm_sgd, aes(x = actual, y = predicted, fill = count)) +
geom_tile() +
geom_text(aes(label = count)) +
scale_fill_gradient(low = "white", high = "blue") +
labs(x = "Actual", y = "Predicted", fill = "Count") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
theme_minimal()
# Plot the confusion matrix
ggplot(cm_sgd, aes(x = actual, y = predicted, fill = count)) +
geom_tile() +
geom_text(aes(label = count)) +
scale_fill_gradient(low = "white", high = "blue") +
labs(x = "Actual", y = "Predicted", fill = "Count") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
theme_minimal()
# Plot the confusion matrix
ggplot(as.data.frame(cm_sgd), aes(x = actual, y = predicted, fill = count)) +
geom_tile(aes(fill = log(count)), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", count)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
labs(fill = "Log(Frequency)")
# Plot the confusion matrix
ggplot(as.data.frame(cm_sgd), aes(x = actual, y = predicted, fill = count)) +
geom_tile(aes(fill = log(count)), colour = "green") +
geom_text(aes(label = sprintf("%1.0f", count)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
labs(fill = "Log(Frequency)")
# Plot the confusion matrix
ggplot(as.data.frame(cm_sgd), aes(x = actual, y = predicted, fill = count)) +
geom_tile(aes(fill = log(count)), colour = "green") +
geom_text(aes(label = sprintf("%1.0f", count)), vjust = 1) +
scale_fill_gradient(low = "purple", high = "steelblue") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
labs(fill = "Log(Frequency)")
geom_tile(aes(fill = count, colour = "green") +
# Plot the confusion matrix
ggplot(as.data.frame(cm_sgd), aes(x = actual, y = predicted, fill = count)) +
geom_text(aes(label = sprintf("%1.0f", count)), vjust = 1) +
scale_fill_gradient(low = "purple", high = "steelblue") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
labs(fill = "Log(Frequency)")
# Plot the confusion matrix
ggplot(cm_sgd, aes(x = actual, y = predicted, fill = count)) +
# Plot the confusion matrix
ggplot(cm_sgd, aes(x = actual, y = predicted, fill = count)) +
geom_tile() +
geom_text(aes(label = count)) +
scale_fill_gradient(low = "white", high = "blue") +
labs(x = "Actual", y = "Predicted", fill = "Count") +
theme_minimal()
# Plot the confusion matrix
ggplot(cm_sgd, aes(x = actual, y = predicted, fill = count)) +
geom_tile() +
geom_text(aes(label = count)) +
scale_fill_gradient(low = "white", high = "blue") +
labs(x = "Actual", y = "Predicted", fill = "Count")
# Plot the confusion matrix
ggplot(cm_sgd, aes(x = actual, y = predicted, fill = count)) +
geom_tile() +
geom_text(aes(label = count)) +
scale_fill_gradient(low = "white", high = "blue") +
labs(x = "Actual", y = "Predicted", fill = "Count") +
theme_minimal()
# Plot the confusion matrix
ggplot(cm_sgd, aes(x = actual, y = predicted, fill = count)) +
geom_tile() +
geom_text(aes(label = count)) +
scale_fill_gradient(low = "grey", high = "blue") +
labs(x = "Actual", y = "Predicted", fill = "Count") +
theme_minimal()
# Plot the confusion matrix
ggplot(cm_sgd, aes(x = actual, y = predicted, fill = count)) +
geom_tile() +
geom_text(aes(label = count)) +
scale_fill_gradient(low = "grey", high = "purple") +
labs(x = "Actual", y = "Predicted", fill = "Count") +
theme_minimal()
# Plot the confusion matrix
ggplot(cm_sgd, aes(x = actual, y = predicted, fill = count)) +
geom_tile() +
geom_text(aes(label = count)) +
scale_fill_gradient(low = "lightgrey", high = "purple") +
labs(x = "Actual", y = "Predicted", fill = "Count") +
theme_minimal()
# Plot the confusion matrix
ggplot(cm_sgd, aes(x = actual, y = predicted, fill = count)) +
geom_tile() +
geom_text(aes(label = count)) +
scale_fill_gradient(low = "lightgrey", high = "purple") +
labs(x = "Actual", y = "Predicted", fill = "Count") +
theme_minimal()
View(first)
View(sgd_first)
View(first)
View(rnn_pred)
View(sgd_first)
sgd_lc <- read.csv("SGD_outputs/learning_curve.csv", header = TRUE)
sgd_cm <- read.csv("SGD_outputs/confusion_matrix.csv", header = TRUE)
rnn_cm <- read.csv("RNN_outputs/confusion_matrix.csv", header = TRUE)
sgd_report <- read.csv("SGD_outputs/classification_report.csv", header = TRUE)
rnn_report <- read.csv("RNN_outputs/classification_report.csv", header = TRUE)
sgd_weights <- read.csv("SGD_outputs/weights.csv", header = TRUE)
rnn_weights <- read.csv("RNN_outputs/weights.csv", header = TRUE)
sgd_top <- read.csv("SGD_outputs/top_bottom_words.csv", header = TRUE)
rnn_top <- read.csv("RNN_outputs/top_bottom_words.csv", header = TRUE)
sgd_roc <- read.csv("SGD_outputs/ROC.csv", header = TRUE)
rnn_roc <- read.csv("RNN_outputs/ROC.csv", header = TRUE)
lines <- readLines("SGD_outputs/dev_predictions.jsonl")
lines <- lapply(lines, fromJSON)
sgd_pred <- bind_rows(lines)
lines2 <- readLines("RNN_outputs/dev_predictions.jsonl")
lines2 <- lapply(lines2, fromJSON)
rnn_pred <- bind_rows(lines2)
View(rnn_cm)
View(rnn_pred)
View(rnn_report)
View(rnn_pred)
View(rnn_pred)
View(rnn_report)
