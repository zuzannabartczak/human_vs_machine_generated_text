{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import learning_curve, ParameterSampler, train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and preprocessing\n",
    "\n",
    "# Load the dataset\n",
    "with open('data_json/SubtaskB/subtaskB_train.jsonl', 'r') as f:\n",
    "    df = pd.read_json(f, lines=True, orient='records')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['model'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder on the string labels\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "# Transform the string labels to integer values\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SGDClassifier model\n",
    "clf = SGDClassifier(loss='log_loss', penalty='l2', alpha=1e-3,\n",
    "                    random_state=42, verbose=0, max_iter=25, tol=None)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Predict the probabilities of the test set labels\n",
    "y_prob = clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.93      0.74      2404\n",
      "           1       0.63      0.57      0.60      2441\n",
      "           2       0.53      0.52      0.52      2204\n",
      "           3       0.55      0.48      0.51      2407\n",
      "           4       0.45      0.28      0.34      2360\n",
      "           5       0.52      0.55      0.53      2390\n",
      "\n",
      "    accuracy                           0.56     14206\n",
      "   macro avg       0.55      0.56      0.54     14206\n",
      "weighted avg       0.55      0.56      0.54     14206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_df = pd.DataFrame(\n",
    "    classification_report(y_test, y_pred, output_dict=True)).transpose()\n",
    "classification_report_df.to_csv(\n",
    "    'statistics/SGD_B_outputs/classification_report.csv', index=False)\n",
    "\n",
    "confusion_df = pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "confusion_df.to_csv('statistics/SGD_B_outputs/confusion_matrix.csv', index=False)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from y_test and y_prob\n",
    "roc = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'prob_0': [prob[0] for prob in y_prob],\n",
    "    'prob_1': [prob[1] for prob in y_prob]\n",
    "})\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "roc.to_csv(\"statistics/SGD_B_outputs/ROC.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients of the trained model\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coef = clf.coef_[0]\n",
    "\n",
    "# Create a dictionary of feature names and coefficients\n",
    "features_coef = dict(zip(feature_names, coef))\n",
    "\n",
    "# Sort the dictionary by coefficient value\n",
    "sorted_features = sorted(features_coef.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print the weights of the top 30 words to a file\n",
    "with open(\"statistics/SGD_B_outputs/top_bottom_words.csv\", \"w\") as f:\n",
    "    f.write(f\"word,weight\\n\")\n",
    "    for word, weight in sorted_features[-30:]:\n",
    "        f.write(f\"{word},{weight}\\n\")\n",
    "\n",
    "    # Print the weights of the bottom 30 words to the same file\n",
    "    for word, weight in sorted_features[:30]:\n",
    "        f.write(f\"{word},{weight}\\n\")\n",
    "\n",
    "# Store all of the weights in a separate csv file\n",
    "weights_df = pd.DataFrame(sorted_features, columns=['word', 'weight'])\n",
    "weights_df.to_csv('statistics/SGD_B_outputs/weights.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zuba1\\Documents\\language_technology\\statistics\\project\\human_vs_machine_generated_text\\SGDmodelB.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zuba1/Documents/language_technology/statistics/project/human_vs_machine_generated_text/SGDmodelB.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cv \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zuba1/Documents/language_technology/statistics/project/human_vs_machine_generated_text/SGDmodelB.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Calculate the learning curve\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zuba1/Documents/language_technology/statistics/project/human_vs_machine_generated_text/SGDmodelB.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_sizes, train_scores, test_scores \u001b[39m=\u001b[39m learning_curve(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zuba1/Documents/language_technology/statistics/project/human_vs_machine_generated_text/SGDmodelB.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     clf, X_train, y_train, train_sizes\u001b[39m=\u001b[39;49mtrain_sizes, cv\u001b[39m=\u001b[39;49mcv, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zuba1/Documents/language_technology/statistics/project/human_vs_machine_generated_text/SGDmodelB.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Create a pandas DataFrame with the learning curve coordinates\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zuba1/Documents/language_technology/statistics/project/human_vs_machine_generated_text/SGDmodelB.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m df_learning_curve \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zuba1/Documents/language_technology/statistics/project/human_vs_machine_generated_text/SGDmodelB.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain_sizes\u001b[39m\u001b[39m'\u001b[39m: train_sizes,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zuba1/Documents/language_technology/statistics/project/human_vs_machine_generated_text/SGDmodelB.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain_scores_mean\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mmean(train_scores, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zuba1/Documents/language_technology/statistics/project/human_vs_machine_generated_text/SGDmodelB.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtest_scores_std\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mstd(test_scores, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zuba1/Documents/language_technology/statistics/project/human_vs_machine_generated_text/SGDmodelB.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m })\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:1597\u001b[0m, in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times, fit_params)\u001b[0m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mfor\u001b[39;00m n_train_samples \u001b[39min\u001b[39;00m train_sizes_abs:\n\u001b[0;32m   1595\u001b[0m         train_test_proportions\u001b[39m.\u001b[39mappend((train[:n_train_samples], test))\n\u001b[1;32m-> 1597\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m   1598\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m   1599\u001b[0m         clone(estimator),\n\u001b[0;32m   1600\u001b[0m         X,\n\u001b[0;32m   1601\u001b[0m         y,\n\u001b[0;32m   1602\u001b[0m         scorer,\n\u001b[0;32m   1603\u001b[0m         train,\n\u001b[0;32m   1604\u001b[0m         test,\n\u001b[0;32m   1605\u001b[0m         verbose,\n\u001b[0;32m   1606\u001b[0m         parameters\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1607\u001b[0m         fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m   1608\u001b[0m         return_train_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1609\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m   1610\u001b[0m         return_times\u001b[39m=\u001b[39;49mreturn_times,\n\u001b[0;32m   1611\u001b[0m     )\n\u001b[0;32m   1612\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m train_test_proportions\n\u001b[0;32m   1613\u001b[0m )\n\u001b[0;32m   1614\u001b[0m results \u001b[39m=\u001b[39m _aggregate_score_dicts(results)\n\u001b[0;32m   1615\u001b[0m train_scores \u001b[39m=\u001b[39m results[\u001b[39m\"\u001b[39m\u001b[39mtrain_scores\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, n_unique_ticks)\u001b[39m.\u001b[39mT\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:894\u001b[0m, in \u001b[0;36mBaseSGDClassifier.fit\u001b[1;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m    892\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_more_validate_params()\n\u001b[1;32m--> 894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m    895\u001b[0m     X,\n\u001b[0;32m    896\u001b[0m     y,\n\u001b[0;32m    897\u001b[0m     alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha,\n\u001b[0;32m    898\u001b[0m     C\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[0;32m    899\u001b[0m     loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss,\n\u001b[0;32m    900\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_rate,\n\u001b[0;32m    901\u001b[0m     coef_init\u001b[39m=\u001b[39;49mcoef_init,\n\u001b[0;32m    902\u001b[0m     intercept_init\u001b[39m=\u001b[39;49mintercept_init,\n\u001b[0;32m    903\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    904\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:683\u001b[0m, in \u001b[0;36mBaseSGDClassifier._fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[39m# Clear iteration count for multiple call to fit.\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_ \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m--> 683\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_partial_fit(\n\u001b[0;32m    684\u001b[0m     X,\n\u001b[0;32m    685\u001b[0m     y,\n\u001b[0;32m    686\u001b[0m     alpha,\n\u001b[0;32m    687\u001b[0m     C,\n\u001b[0;32m    688\u001b[0m     loss,\n\u001b[0;32m    689\u001b[0m     learning_rate,\n\u001b[0;32m    690\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    691\u001b[0m     classes,\n\u001b[0;32m    692\u001b[0m     sample_weight,\n\u001b[0;32m    693\u001b[0m     coef_init,\n\u001b[0;32m    694\u001b[0m     intercept_init,\n\u001b[0;32m    695\u001b[0m )\n\u001b[0;32m    697\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    698\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol \u001b[39m>\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf\n\u001b[0;32m    700\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter\n\u001b[0;32m    701\u001b[0m ):\n\u001b[0;32m    702\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    703\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMaximum number of iteration reached before \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    704\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mconvergence. Consider increasing max_iter to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    705\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mimprove the fit.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    706\u001b[0m         ConvergenceWarning,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:617\u001b[0m, in \u001b[0;36mBaseSGDClassifier._partial_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[39m# delegate to concrete training procedure\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \u001b[39mif\u001b[39;00m n_classes \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 617\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_multiclass(\n\u001b[0;32m    618\u001b[0m         X,\n\u001b[0;32m    619\u001b[0m         y,\n\u001b[0;32m    620\u001b[0m         alpha\u001b[39m=\u001b[39;49malpha,\n\u001b[0;32m    621\u001b[0m         C\u001b[39m=\u001b[39;49mC,\n\u001b[0;32m    622\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m    623\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    624\u001b[0m         max_iter\u001b[39m=\u001b[39;49mmax_iter,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[0;32m    626\u001b[0m \u001b[39melif\u001b[39;00m n_classes \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_binary(\n\u001b[0;32m    628\u001b[0m         X,\n\u001b[0;32m    629\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    634\u001b[0m         max_iter\u001b[39m=\u001b[39mmax_iter,\n\u001b[0;32m    635\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:760\u001b[0m, in \u001b[0;36mBaseSGDClassifier._fit_multiclass\u001b[1;34m(self, X, y, alpha, C, learning_rate, sample_weight, max_iter)\u001b[0m\n\u001b[0;32m    758\u001b[0m random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n\u001b[0;32m    759\u001b[0m seeds \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_))\n\u001b[1;32m--> 760\u001b[0m result \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    761\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, require\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msharedmem\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    762\u001b[0m )(\n\u001b[0;32m    763\u001b[0m     delayed(fit_binary)(\n\u001b[0;32m    764\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    765\u001b[0m         i,\n\u001b[0;32m    766\u001b[0m         X,\n\u001b[0;32m    767\u001b[0m         y,\n\u001b[0;32m    768\u001b[0m         alpha,\n\u001b[0;32m    769\u001b[0m         C,\n\u001b[0;32m    770\u001b[0m         learning_rate,\n\u001b[0;32m    771\u001b[0m         max_iter,\n\u001b[0;32m    772\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_expanded_class_weight[i],\n\u001b[0;32m    773\u001b[0m         \u001b[39m1.0\u001b[39;49m,\n\u001b[0;32m    774\u001b[0m         sample_weight,\n\u001b[0;32m    775\u001b[0m         validation_mask\u001b[39m=\u001b[39;49mvalidation_mask,\n\u001b[0;32m    776\u001b[0m         random_state\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    777\u001b[0m     )\n\u001b[0;32m    778\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, seed \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(seeds)\n\u001b[0;32m    779\u001b[0m )\n\u001b[0;32m    781\u001b[0m \u001b[39m# take the maximum of n_iter_ over every binary fit\u001b[39;00m\n\u001b[0;32m    782\u001b[0m n_iter_ \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:446\u001b[0m, in \u001b[0;36mfit_binary\u001b[1;34m(est, i, X, y, alpha, C, learning_rate, max_iter, pos_weight, neg_weight, sample_weight, validation_mask, random_state)\u001b[0m\n\u001b[0;32m    442\u001b[0m seed \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39mrandint(MAX_INT)\n\u001b[0;32m    444\u001b[0m tol \u001b[39m=\u001b[39m est\u001b[39m.\u001b[39mtol \u001b[39mif\u001b[39;00m est\u001b[39m.\u001b[39mtol \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf\n\u001b[1;32m--> 446\u001b[0m coef, intercept, average_coef, average_intercept, n_iter_ \u001b[39m=\u001b[39m _plain_sgd(\n\u001b[0;32m    447\u001b[0m     coef,\n\u001b[0;32m    448\u001b[0m     intercept,\n\u001b[0;32m    449\u001b[0m     average_coef,\n\u001b[0;32m    450\u001b[0m     average_intercept,\n\u001b[0;32m    451\u001b[0m     est\u001b[39m.\u001b[39;49mloss_function_,\n\u001b[0;32m    452\u001b[0m     penalty_type,\n\u001b[0;32m    453\u001b[0m     alpha,\n\u001b[0;32m    454\u001b[0m     C,\n\u001b[0;32m    455\u001b[0m     est\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[0;32m    456\u001b[0m     dataset,\n\u001b[0;32m    457\u001b[0m     validation_mask,\n\u001b[0;32m    458\u001b[0m     est\u001b[39m.\u001b[39;49mearly_stopping,\n\u001b[0;32m    459\u001b[0m     validation_score_cb,\n\u001b[0;32m    460\u001b[0m     \u001b[39mint\u001b[39;49m(est\u001b[39m.\u001b[39;49mn_iter_no_change),\n\u001b[0;32m    461\u001b[0m     max_iter,\n\u001b[0;32m    462\u001b[0m     tol,\n\u001b[0;32m    463\u001b[0m     \u001b[39mint\u001b[39;49m(est\u001b[39m.\u001b[39;49mfit_intercept),\n\u001b[0;32m    464\u001b[0m     \u001b[39mint\u001b[39;49m(est\u001b[39m.\u001b[39;49mverbose),\n\u001b[0;32m    465\u001b[0m     \u001b[39mint\u001b[39;49m(est\u001b[39m.\u001b[39;49mshuffle),\n\u001b[0;32m    466\u001b[0m     seed,\n\u001b[0;32m    467\u001b[0m     pos_weight,\n\u001b[0;32m    468\u001b[0m     neg_weight,\n\u001b[0;32m    469\u001b[0m     learning_rate_type,\n\u001b[0;32m    470\u001b[0m     est\u001b[39m.\u001b[39;49meta0,\n\u001b[0;32m    471\u001b[0m     est\u001b[39m.\u001b[39;49mpower_t,\n\u001b[0;32m    472\u001b[0m     \u001b[39m0\u001b[39;49m,\n\u001b[0;32m    473\u001b[0m     est\u001b[39m.\u001b[39;49mt_,\n\u001b[0;32m    474\u001b[0m     intercept_decay,\n\u001b[0;32m    475\u001b[0m     est\u001b[39m.\u001b[39;49maverage,\n\u001b[0;32m    476\u001b[0m )\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m est\u001b[39m.\u001b[39maverage:\n\u001b[0;32m    479\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(est\u001b[39m.\u001b[39mclasses_) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Define the parameters\n",
    "# train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "# cv = 5\n",
    "\n",
    "# # Calculate the learning curve\n",
    "# train_sizes, train_scores, test_scores = learning_curve(\n",
    "#     clf, X_train, y_train, train_sizes=train_sizes, cv=cv, verbose=0)\n",
    "\n",
    "# # Create a pandas DataFrame with the learning curve coordinates\n",
    "# df_learning_curve = pd.DataFrame({\n",
    "#     'train_sizes': train_sizes,\n",
    "#     'train_scores_mean': np.mean(train_scores, axis=1),\n",
    "#     'test_scores_mean': np.mean(test_scores, axis=1),\n",
    "#     'train_scores_std': np.std(train_scores, axis=1),\n",
    "#     'test_scores_std': np.std(test_scores, axis=1)\n",
    "# })\n",
    "\n",
    "# # Save the DataFrame to a csv file\n",
    "# df_learning_curve.to_csv('statistics/SGD_B_outputs/learning_curve.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:\tReg.dampening:\tTraining set accuracy:\n",
      "0.03071\t\t0.00010\t\t83.9%\n",
      "0.03071\t\t3.00000\t\t21.7%\n",
      "0.09655\t\t3.00000\t\t17.6%\n",
      "0.00031\t\t0.00977\t\t55.6%\n",
      "3.00000\t\t0.00977\t\t28.0%\n",
      "0.30353\t\t0.30353\t\t20.1%\n",
      "0.00311\t\t3.00000\t\t28.2%\n",
      "0.00031\t\t0.00010\t\t64.9%\n",
      "0.09655\t\t0.95425\t\t24.6%\n",
      "0.30353\t\t0.09655\t\t21.5%\n",
      "0.00010\t\t0.00010\t\t53.2%\n",
      "0.95425\t\t0.00031\t\t61.7%\n",
      "0.30353\t\t0.00977\t\t43.3%\n",
      "0.03071\t\t0.95425\t\t28.2%\n",
      "0.00977\t\t0.00977\t\t65.3%\n",
      "Best parameters: 0.03071, 0.00010\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "parameter_distribution = {'learning_rate': np.exp(np.linspace(np.log(0.0001), np.log(3), 10)),\n",
    "                          'reguliser_dampening': np.exp(np.linspace(np.log(0.0001), np.log(3), 10))}\n",
    "\n",
    "# Placeholder to make future comparissons easier\n",
    "best_hyperparameters = None\n",
    "print(\"Learning rate:\\tReg.dampening:\\tTraining set accuracy:\")\n",
    "\n",
    "for hyperparameters in ParameterSampler(parameter_distribution, n_iter=15):\n",
    "  # Set up the classifier\n",
    "  reguliser_dampening = hyperparameters['reguliser_dampening']\n",
    "  learning_rate = hyperparameters['learning_rate']\n",
    "  model = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                        alpha=reguliser_dampening, verbose=0,\n",
    "                        learning_rate='constant', eta0=learning_rate)\n",
    "\n",
    "  # Train the classifier\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Calculate the training accuracy\n",
    "  training_accuracy = np.sum(model.predict(X_train) == y_train)/len(y_train)\n",
    "\n",
    "  # Store the hyperparameters if they are better than what we have found before\n",
    "  if best_hyperparameters is None or best_hyperparameters[1] < training_accuracy:\n",
    "    best_hyperparameters = (hyperparameters, training_accuracy)\n",
    "  print(\"%.5f\\t\\t%.5f\\t\\t%.1f%%\" % (\n",
    "      hyperparameters['learning_rate'], hyperparameters['reguliser_dampening'], 100*training_accuracy))\n",
    "\n",
    "best_learning_rate = best_hyperparameters[0]['learning_rate']\n",
    "best_reguliser_dampening = best_hyperparameters[0]['reguliser_dampening']\n",
    "print(\"Best parameters: %.5f, %.5f\" %\n",
    "      (best_learning_rate, best_reguliser_dampening))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 17.35, NNZs: 107689, Bias: -0.224802, T: 56821, Avg. loss: 0.227361\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 23.97, NNZs: 110026, Bias: -0.219274, T: 113642, Avg. loss: 0.125010\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 27.07, NNZs: 110619, Bias: -0.208833, T: 170463, Avg. loss: 0.096802\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 28.74, NNZs: 110933, Bias: -0.202076, T: 227284, Avg. loss: 0.085154\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 29.72, NNZs: 111144, Bias: -0.199312, T: 284105, Avg. loss: 0.079047\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 30.39, NNZs: 111311, Bias: -0.197470, T: 340926, Avg. loss: 0.075458\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 30.90, NNZs: 111408, Bias: -0.197470, T: 397747, Avg. loss: 0.073059\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 31.25, NNZs: 111491, Bias: -0.203305, T: 454568, Avg. loss: 0.071286\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 31.48, NNZs: 111528, Bias: -0.195627, T: 511389, Avg. loss: 0.069913\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 31.70, NNZs: 111592, Bias: -0.202998, T: 568210, Avg. loss: 0.069095\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 31.92, NNZs: 111609, Bias: -0.197777, T: 625031, Avg. loss: 0.068296\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 32.03, NNZs: 111652, Bias: -0.194706, T: 681852, Avg. loss: 0.067663\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 32.17, NNZs: 111771, Bias: -0.199927, T: 738673, Avg. loss: 0.067216\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 32.25, NNZs: 111820, Bias: -0.196548, T: 795494, Avg. loss: 0.066737\n",
      "Total training time: 0.78 seconds.\n",
      "Convergence after 14 epochs took 0.78 seconds\n",
      "-- Epoch 1\n",
      "Norm: 10.46, NNZs: 102495, Bias: -0.874640, T: 56821, Avg. loss: 0.333439\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 15.27, NNZs: 109156, Bias: -0.964316, T: 113642, Avg. loss: 0.279139\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 18.15, NNZs: 111898, Bias: -1.007618, T: 170463, Avg. loss: 0.262198\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 20.15, NNZs: 113270, Bias: -1.041707, T: 227284, Avg. loss: 0.252725\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 21.75, NNZs: 114097, Bias: -1.060440, T: 284105, Avg. loss: 0.246324\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 22.99, NNZs: 114829, Bias: -1.076103, T: 340926, Avg. loss: 0.241421\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 23.95, NNZs: 115533, Bias: -1.090230, T: 397747, Avg. loss: 0.237706\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 24.74, NNZs: 116138, Bias: -1.104971, T: 454568, Avg. loss: 0.234824\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 25.41, NNZs: 116478, Bias: -1.112648, T: 511389, Avg. loss: 0.232635\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 25.96, NNZs: 116678, Bias: -1.124011, T: 568210, Avg. loss: 0.230736\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 26.38, NNZs: 116935, Bias: -1.124625, T: 625031, Avg. loss: 0.229195\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 26.71, NNZs: 117237, Bias: -1.136296, T: 681852, Avg. loss: 0.227902\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 27.03, NNZs: 117450, Bias: -1.135067, T: 738673, Avg. loss: 0.227059\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 27.28, NNZs: 117583, Bias: -1.143973, T: 795494, Avg. loss: 0.226268\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 27.46, NNZs: 117747, Bias: -1.144280, T: 852315, Avg. loss: 0.225613\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 27.63, NNZs: 117865, Bias: -1.146737, T: 909136, Avg. loss: 0.225000\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 27.74, NNZs: 117941, Bias: -1.153801, T: 965957, Avg. loss: 0.224600\n",
      "Total training time: 0.97 seconds.\n",
      "Convergence after 17 epochs took 0.97 seconds\n",
      "-- Epoch 1\n",
      "Norm: 8.71, NNZs: 107828, Bias: -0.768381, T: 56821, Avg. loss: 0.328073\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12.67, NNZs: 116419, Bias: -0.827039, T: 113642, Avg. loss: 0.291171\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 16.12, NNZs: 120062, Bias: -0.832567, T: 170463, Avg. loss: 0.276759\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 19.03, NNZs: 121780, Bias: -0.834102, T: 227284, Avg. loss: 0.265484\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 21.40, NNZs: 122878, Bias: -0.825503, T: 284105, Avg. loss: 0.256149\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 23.31, NNZs: 123584, Bias: -0.815369, T: 340926, Avg. loss: 0.248784\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 24.78, NNZs: 123998, Bias: -0.799706, T: 397747, Avg. loss: 0.243144\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 25.96, NNZs: 124339, Bias: -0.801549, T: 454568, Avg. loss: 0.238871\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 26.85, NNZs: 124526, Bias: -0.793871, T: 511389, Avg. loss: 0.235293\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 27.57, NNZs: 124688, Bias: -0.784044, T: 568210, Avg. loss: 0.232731\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 28.14, NNZs: 124847, Bias: -0.784965, T: 625031, Avg. loss: 0.230748\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 28.59, NNZs: 124973, Bias: -0.782201, T: 681852, Avg. loss: 0.229136\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 28.95, NNZs: 125182, Bias: -0.769917, T: 738673, Avg. loss: 0.228003\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 29.23, NNZs: 125317, Bias: -0.771760, T: 795494, Avg. loss: 0.226876\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 29.48, NNZs: 125494, Bias: -0.771760, T: 852315, Avg. loss: 0.226024\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 29.66, NNZs: 125522, Bias: -0.770224, T: 909136, Avg. loss: 0.225371\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 29.81, NNZs: 125554, Bias: -0.765925, T: 965957, Avg. loss: 0.224621\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 29.92, NNZs: 125604, Bias: -0.761932, T: 1022778, Avg. loss: 0.224471\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 30.00, NNZs: 125682, Bias: -0.766539, T: 1079599, Avg. loss: 0.223955\n",
      "Total training time: 1.57 seconds.\n",
      "Convergence after 19 epochs took 1.58 seconds\n",
      "-- Epoch 1\n",
      "Norm: 6.48, NNZs: 110797, Bias: -0.928998, T: 56821, Avg. loss: 0.357302\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.27, NNZs: 122834, Bias: -1.015295, T: 113642, Avg. loss: 0.324064\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 11.87, NNZs: 129056, Bias: -1.055834, T: 170463, Avg. loss: 0.315578\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.12, NNZs: 132613, Bias: -1.089922, T: 227284, Avg. loss: 0.308941\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 16.03, NNZs: 135050, Bias: -1.110499, T: 284105, Avg. loss: 0.303274\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 17.63, NNZs: 136843, Bias: -1.136296, T: 340926, Avg. loss: 0.298637\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 18.99, NNZs: 137835, Bias: -1.150422, T: 397747, Avg. loss: 0.294663\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 20.14, NNZs: 139322, Bias: -1.164857, T: 454568, Avg. loss: 0.291410\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 21.09, NNZs: 139704, Bias: -1.180826, T: 511389, Avg. loss: 0.288587\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 21.90, NNZs: 140146, Bias: -1.189732, T: 568210, Avg. loss: 0.286314\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 22.58, NNZs: 140810, Bias: -1.198638, T: 625031, Avg. loss: 0.284390\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 23.15, NNZs: 141270, Bias: -1.209387, T: 681852, Avg. loss: 0.282820\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 23.64, NNZs: 141775, Bias: -1.213379, T: 738673, Avg. loss: 0.281430\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 24.03, NNZs: 142032, Bias: -1.222900, T: 795494, Avg. loss: 0.280247\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 24.38, NNZs: 142215, Bias: -1.224435, T: 852315, Avg. loss: 0.279204\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 24.67, NNZs: 142480, Bias: -1.228735, T: 909136, Avg. loss: 0.278450\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 24.89, NNZs: 142561, Bias: -1.233034, T: 965957, Avg. loss: 0.277772\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 25.09, NNZs: 142639, Bias: -1.235491, T: 1022778, Avg. loss: 0.277163\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 25.25, NNZs: 142705, Bias: -1.239791, T: 1079599, Avg. loss: 0.276752\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 25.38, NNZs: 142750, Bias: -1.241019, T: 1136420, Avg. loss: 0.276371\n",
      "Total training time: 1.49 seconds.\n",
      "Convergence after 20 epochs took 1.49 seconds\n",
      "-- Epoch 1\n",
      "Norm: 6.00, NNZs: 123534, Bias: -0.780973, T: 56821, Avg. loss: 0.348936\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.78, NNZs: 133796, Bias: -0.869727, T: 113642, Avg. loss: 0.322904\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.52, NNZs: 139805, Bias: -0.889996, T: 170463, Avg. loss: 0.317762\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.04, NNZs: 143914, Bias: -0.893067, T: 227284, Avg. loss: 0.314160\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.33, NNZs: 146358, Bias: -0.889074, T: 284105, Avg. loss: 0.311393\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 13.42, NNZs: 148632, Bias: -0.883854, T: 340926, Avg. loss: 0.308986\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 14.34, NNZs: 149972, Bias: -0.880476, T: 397747, Avg. loss: 0.307018\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 15.12, NNZs: 151139, Bias: -0.882625, T: 454568, Avg. loss: 0.305391\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 15.77, NNZs: 151743, Bias: -0.876790, T: 511389, Avg. loss: 0.303963\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 16.31, NNZs: 152350, Bias: -0.874948, T: 568210, Avg. loss: 0.302843\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 16.76, NNZs: 152745, Bias: -0.872798, T: 625031, Avg. loss: 0.301922\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 17.14, NNZs: 153330, Bias: -0.872798, T: 681852, Avg. loss: 0.301094\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 17.45, NNZs: 153679, Bias: -0.868498, T: 738673, Avg. loss: 0.300479\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 17.72, NNZs: 153832, Bias: -0.875255, T: 795494, Avg. loss: 0.299951\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 17.94, NNZs: 154188, Bias: -0.866041, T: 852315, Avg. loss: 0.299432\n",
      "Total training time: 0.75 seconds.\n",
      "Convergence after 15 epochs took 0.75 seconds\n",
      "-- Epoch 1\n",
      "Norm: 6.63, NNZs: 158090, Bias: -1.013760, T: 56821, Avg. loss: 0.357631\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.32, NNZs: 166455, Bias: -1.167928, T: 113642, Avg. loss: 0.317428\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.71, NNZs: 168947, Bias: -1.273572, T: 170463, Avg. loss: 0.304066\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.63, NNZs: 169936, Bias: -1.363248, T: 227284, Avg. loss: 0.293077\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 19.10, NNZs: 170423, Bias: -1.443402, T: 284105, Avg. loss: 0.283857\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 21.18, NNZs: 170783, Bias: -1.499603, T: 340926, Avg. loss: 0.276159\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 22.92, NNZs: 171009, Bias: -1.559182, T: 397747, Avg. loss: 0.269843\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 24.30, NNZs: 171262, Bias: -1.610162, T: 454568, Avg. loss: 0.264628\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 25.43, NNZs: 171420, Bias: -1.651928, T: 511389, Avg. loss: 0.260482\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 26.32, NNZs: 171549, Bias: -1.689395, T: 568210, Avg. loss: 0.257204\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 27.06, NNZs: 171661, Bias: -1.716421, T: 625031, Avg. loss: 0.254550\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 27.67, NNZs: 171719, Bias: -1.738532, T: 681852, Avg. loss: 0.252373\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 28.14, NNZs: 171757, Bias: -1.761565, T: 738673, Avg. loss: 0.250697\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 28.52, NNZs: 171815, Bias: -1.775385, T: 795494, Avg. loss: 0.249351\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 28.81, NNZs: 171867, Bias: -1.793504, T: 852315, Avg. loss: 0.248172\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 29.07, NNZs: 171926, Bias: -1.797804, T: 909136, Avg. loss: 0.247376\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 29.27, NNZs: 171953, Bias: -1.813773, T: 965957, Avg. loss: 0.246745\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 29.45, NNZs: 171988, Bias: -1.824829, T: 1022778, Avg. loss: 0.245955\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 29.61, NNZs: 172041, Bias: -1.834657, T: 1079599, Avg. loss: 0.245562\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 29.74, NNZs: 172065, Bias: -1.831278, T: 1136420, Avg. loss: 0.245179\n",
      "Total training time: 1.04 seconds.\n",
      "Convergence after 20 epochs took 1.04 seconds\n",
      "Test set accuracy 73.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    6.5s finished\n"
     ]
    }
   ],
   "source": [
    "# Set up the classifier\n",
    "model = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                      alpha=best_reguliser_dampening, verbose=1,\n",
    "                      learning_rate='constant', eta0=best_learning_rate)\n",
    "\n",
    "# Train on all the non-test data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Run prediction on the test set\n",
    "test_accuracy = np.sum(model.predict(X_test) == y_test)/len(y_test)\n",
    "\n",
    "print(\"Test set accuracy %.1f%%\" % (100*test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset\n",
    "with open(\"data_json/SubtaskB/subtaskB_dev.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Parse each line as a separate JSON object\n",
    "data = []\n",
    "for line in lines:\n",
    "    obj = json.loads(line)\n",
    "    data.append(obj)\n",
    "\n",
    "# Convert the list of JSON objects to a pandas DataFrame\n",
    "dev_df = pd.DataFrame(data)\n",
    "\n",
    "# Vectorize the text data using TfidfVectorizer\n",
    "X_dev = vectorizer.transform(dev_df['text'])\n",
    "y_dev = label_encoder.transform(dev_df['model'])\n",
    "\n",
    "# Predict the labels for the new dataset\n",
    "new_predictions = clf.predict(X_dev)\n",
    "\n",
    "# Store the predictions in a separate jsonl file\n",
    "predictions = list(zip(dev_df['id'], new_predictions))\n",
    "predictions_df = pd.DataFrame(predictions, columns=['id', 'label'])\n",
    "predictions_df.to_json('statistics/SGD_B_outputs/dev_predictions.jsonl',\n",
    "                       lines=True, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.87       500\n",
      "           1       0.57      0.49      0.53       500\n",
      "           2       0.27      0.09      0.13       500\n",
      "           3       0.36      0.65      0.47       500\n",
      "           4       0.17      0.18      0.18       500\n",
      "           5       0.65      0.44      0.53       500\n",
      "\n",
      "    accuracy                           0.48      3000\n",
      "   macro avg       0.47      0.48      0.45      3000\n",
      "weighted avg       0.47      0.47      0.45      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report2_df = pd.DataFrame(\n",
    "    classification_report(y_dev, new_predictions, output_dict=True)).transpose()\n",
    "classification_report2_df.to_csv(\n",
    "    'statistics/SGD_B_outputs/classification_report2.csv', index=False)\n",
    "\n",
    "print(classification_report(y_dev, new_predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
