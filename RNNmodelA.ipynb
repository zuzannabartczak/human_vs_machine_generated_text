{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split, learning_curve, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and preprocessing\n",
    "\n",
    "# Load the dataset\n",
    "with open('data_json/SubtaskA/subtaskA_train_monolingual.jsonl', 'r') as f:\n",
    "    df = pd.read_json(f, lines=True, orient='records')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize training and testing sets\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(sequences_train, maxlen=100, truncating='post')\n",
    "\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(sequences_test, maxlen=100, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "2994/2994 [==============================] - 99s 31ms/step - loss: 0.5627 - accuracy: 0.7090\n",
      "Epoch 2/7\n",
      "2994/2994 [==============================] - 130s 43ms/step - loss: 0.6001 - accuracy: 0.6806\n",
      "Epoch 3/7\n",
      "2994/2994 [==============================] - 113s 38ms/step - loss: 0.5216 - accuracy: 0.7542\n",
      "Epoch 4/7\n",
      "2994/2994 [==============================] - 115s 39ms/step - loss: 0.5001 - accuracy: 0.7661\n",
      "Epoch 5/7\n",
      "2994/2994 [==============================] - 117s 39ms/step - loss: 0.5419 - accuracy: 0.7387\n",
      "Epoch 6/7\n",
      "2994/2994 [==============================] - 106s 36ms/step - loss: 0.5117 - accuracy: 0.7618\n",
      "Epoch 7/7\n",
      "2994/2994 [==============================] - 90s 30ms/step - loss: 0.5182 - accuracy: 0.7551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1cd66a9faf0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=7, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749/749 [==============================] - 6s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict the probabilities of the test set\n",
    "probabilities = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred = [1 if prob > 0.5 else 0 for prob in probabilities]\n",
    "\n",
    "probabilities = list(zip(y_test, probabilities.flatten()))\n",
    "probabilities_df = pd.DataFrame(\n",
    "    probabilities, columns=['actual', 'predicted'])\n",
    "probabilities_df.to_csv('RNN_outputs/ROC.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n",
    "classification_report_df.to_csv('RNN_outputs/classification_report.csv', index=False)\n",
    "\n",
    "confusion_df = pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "confusion_df.to_csv('RNN_outputs/confusion_matrix.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of the embedding layer\n",
    "embedding_weights = model.get_weights()[0]\n",
    "\n",
    "# Get the word index from the tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Create a list of (word, weight) pairs\n",
    "word_weights = []\n",
    "for word, index in word_index.items():\n",
    "    if index < embedding_weights.shape[0]:\n",
    "        weight = np.linalg.norm(embedding_weights[index])\n",
    "        word_weights.append((word, weight))\n",
    "\n",
    "# Store the weights in a csv file\n",
    "sorted_features = sorted(word_weights, key=lambda x: x[1])\n",
    "\n",
    "# Print the weights of the top 30 words to a file\n",
    "with open(\"RNN_outputs/top_bottom_words.csv\", \"w\") as f:\n",
    "    f.write(f\"word,weight\\n\")\n",
    "    for word, weight in sorted_features[-30:]:\n",
    "        f.write(f\"{word},{weight}\\n\")\n",
    "\n",
    "    # Print the weights of the bottom 30 words to the same file\n",
    "    for word, weight in sorted_features[:30]:\n",
    "        f.write(f\"{word},{weight}\\n\")\n",
    "\n",
    "# Store all of the weights in a separate csv file\n",
    "weights_df = pd.DataFrame(sorted_features, columns=['word', 'weight'])\n",
    "weights_df.to_csv('RNN_outputs/weights.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load the dev set\n",
    "with open(\"data_json/SubtaskA/subtaskA_dev_monolingual.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Parse each line as a separate JSON object\n",
    "data = []\n",
    "for line in lines:\n",
    "    obj = json.loads(line)\n",
    "    data.append(obj)\n",
    "\n",
    "# Convert the list of JSON objects to a pandas DataFrame\n",
    "dev_df = pd.DataFrame(data)\n",
    "\n",
    "# Tokenize and pad the sentences in the dev set\n",
    "dev_sequences = tokenizer.texts_to_sequences(dev_df['text'])\n",
    "dev_padded_sequences = pad_sequences(dev_sequences, maxlen=100, truncating='post')\n",
    "\n",
    "# Predict the labels for the dev set\n",
    "predictions = model.predict(dev_padded_sequences)\n",
    "\n",
    "# Convert the predicted probabilities to binary labels\n",
    "pred_labels = [1 if p >= 0.5 else 0 for p in predictions]\n",
    "\n",
    "# Store the predictions in a separate jsonl file\n",
    "predictions = list(zip(dev_df['id'], pred_labels))\n",
    "predictions_df = pd.DataFrame(predictions, columns=['id', 'label'])\n",
    "predictions_df.to_json('RNN_outputs/dev_predictions.jsonl', lines=True, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report2_df = pd.DataFrame(\n",
    "    classification_report(dev_df['label'], pred_labels, output_dict=True)).transpose()\n",
    "classification_report2_df.to_csv(\n",
    "    'RNN_outputs/classification_report2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the train sizes\n",
    "# train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# # Define lists to store the train and validation scores for each size\n",
    "# train_scores = []\n",
    "# val_scores = []\n",
    "\n",
    "# # Loop over the train sizes\n",
    "# for size in train_sizes:\n",
    "#     # Split the training set into a smaller training set and a validation set\n",
    "#     X_train_small, X_val, y_train_small, y_val = train_test_split(\n",
    "#         X_train, y_train, train_size=size, random_state=42)\n",
    "    \n",
    "#     # Train the model on the smaller training set\n",
    "#     model.fit(X_train_small, y_train_small, epochs=7, verbose=0)\n",
    "    \n",
    "#     # Evaluate the model on the smaller training set and the validation set\n",
    "#     train_loss, train_acc = model.evaluate(X_train_small, y_train_small, verbose=0)\n",
    "#     val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    \n",
    "#     # Append the scores to the lists\n",
    "#     train_scores.append(train_acc)\n",
    "#     val_scores.append(val_acc)\n",
    "\n",
    "# # Convert the lists to arrays\n",
    "# train_scores = np.array(train_scores)\n",
    "# val_scores = np.array(val_scores)\n",
    "\n",
    "# # Calculate the mean and standard deviation of the train and validation scores\n",
    "# df_learning_curve = pd.DataFrame({\n",
    "#     'train_sizes': train_sizes,\n",
    "#     'train_scores_mean': train_scores.mean(axis=1),\n",
    "#     'test_scores_mean': val_scores.mean(axis=1),\n",
    "#     'train_scores_std': train_scores.std(axis=1),\n",
    "#     'test_scores_std': val_scores.std(axis=1)\n",
    "# })\n",
    "\n",
    "# # Save the learning curve to a csv file\n",
    "# df_learning_curve.to_csv(\"RNN_outputs/learning_curve.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning / grid search\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import SimpleRNN, Dense\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Function to create model, required for KerasClassifier\n",
    "# def create_model(units=50, optimizer='adam'):\n",
    "#     model = Sequential()\n",
    "#     model.add(SimpleRNN(units, input_shape=(100, 1)))  # Assume input sequences of length 100\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Create the KerasClassifier wrapper\n",
    "# model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# # Define the grid search parameters\n",
    "# param_grid = {\n",
    "#     'units': [50, 100, 150],\n",
    "#     'optimizer': ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'],\n",
    "#     'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "#     'epochs': [10, 50, 100]\n",
    "# }\n",
    "\n",
    "# # Create Grid Search\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "# grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# # Report Results\n",
    "# print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
