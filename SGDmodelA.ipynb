{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import learning_curve, ParameterSampler, train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and preprocessing\n",
    "\n",
    "# Load the dataset\n",
    "with open('data_json/SubtaskA/subtaskA_train_monolingual.jsonl', 'r') as f:\n",
    "    df = pd.read_json(f, lines=True, orient='records')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train the SGDClassifier model\n",
    "clf = SGDClassifier(loss='log', penalty='l2', alpha=1e-3,\n",
    "                    random_state=42, verbose=0, max_iter=15, tol=None)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Predict the probabilities of the test set labels\n",
    "y_prob = clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.90      0.77     12496\n",
      "           1       0.82      0.51      0.63     11456\n",
      "\n",
      "    accuracy                           0.71     23952\n",
      "   macro avg       0.75      0.71      0.70     23952\n",
      "weighted avg       0.74      0.71      0.70     23952\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_df = pd.DataFrame(\n",
    "    classification_report(y_test, y_pred, output_dict=True)).transpose()\n",
    "classification_report_df.to_csv(\n",
    "    'SGD_outputs/classification_report.csv', index=False)\n",
    "\n",
    "confusion_df = pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "confusion_df.to_csv('SGD_outputs/confusion_matrix.csv', index=False)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from y_test and y_prob\n",
    "roc = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'prob_0': [prob[0] for prob in y_prob],\n",
    "    'prob_1': [prob[1] for prob in y_prob]\n",
    "})\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "roc.to_csv(\"SGD_outputs/ROC.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients of the trained model\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coef = clf.coef_[0]\n",
    "\n",
    "# Create a dictionary of feature names and coefficients\n",
    "features_coef = dict(zip(feature_names, coef))\n",
    "\n",
    "# Sort the dictionary by coefficient value\n",
    "sorted_features = sorted(features_coef.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print the weights of the top 30 words to a file\n",
    "with open(\"SGD_outputs/top_bottom_words.csv\", \"w\") as f:\n",
    "    f.write(f\"word,weight\\n\")\n",
    "    for word, weight in sorted_features[-30:]:\n",
    "        f.write(f\"{word},{weight}\\n\")\n",
    "\n",
    "    # Print the weights of the bottom 30 words to the same file\n",
    "    for word, weight in sorted_features[:30]:\n",
    "        f.write(f\"{word},{weight}\\n\")\n",
    "\n",
    "# Store all of the weights in a separate csv file\n",
    "weights_df = pd.DataFrame(sorted_features, columns=['word', 'weight'])\n",
    "weights_df.to_csv('SGD_outputs/weights.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "cv = 5\n",
    "\n",
    "# Calculate the learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    clf, X_train, y_train, train_sizes=train_sizes, cv=cv, verbose=0)\n",
    "\n",
    "# Create a pandas DataFrame with the learning curve coordinates\n",
    "df_learning_curve = pd.DataFrame({\n",
    "    'train_sizes': train_sizes,\n",
    "    'train_scores_mean': np.mean(train_scores, axis=1),\n",
    "    'test_scores_mean': np.mean(test_scores, axis=1),\n",
    "    'train_scores_std': np.std(train_scores, axis=1),\n",
    "    'test_scores_std': np.std(test_scores, axis=1)\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a csv file\n",
    "df_learning_curve.to_csv('SGD_outputs/learning_curve.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:\tReg.dampening:\tTraining set accuracy:\n",
      "0.00311\t\t0.03071\t\t53.1%\n",
      "3.00000\t\t0.30353\t\t53.1%\n",
      "0.00977\t\t0.00010\t\t88.8%\n",
      "3.00000\t\t0.03071\t\t53.1%\n",
      "0.00311\t\t0.00010\t\t88.1%\n",
      "0.00031\t\t0.00099\t\t65.8%\n",
      "0.03071\t\t0.95425\t\t53.1%\n",
      "0.00099\t\t0.00099\t\t67.5%\n",
      "0.00311\t\t0.00031\t\t83.1%\n",
      "0.00010\t\t0.00010\t\t75.4%\n",
      "0.00977\t\t0.00099\t\t68.1%\n",
      "3.00000\t\t0.95425\t\t53.1%\n",
      "0.00977\t\t0.00311\t\t59.2%\n",
      "0.00031\t\t0.00311\t\t59.9%\n",
      "0.00031\t\t0.09655\t\t53.1%\n",
      "Best parameters: 0.00977, 0.00010\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "parameter_distribution = {'learning_rate': np.exp(np.linspace(np.log(0.0001), np.log(3), 10)),\n",
    "                          'reguliser_dampening': np.exp(np.linspace(np.log(0.0001), np.log(3), 10))}\n",
    "\n",
    "# Placeholder to make future comparissons easier\n",
    "best_hyperparameters = None\n",
    "print(\"Learning rate:\\tReg.dampening:\\tTraining set accuracy:\")\n",
    "\n",
    "for hyperparameters in ParameterSampler(parameter_distribution, n_iter=15):\n",
    "  # Set up the classifier\n",
    "  reguliser_dampening = hyperparameters['reguliser_dampening']\n",
    "  learning_rate = hyperparameters['learning_rate']\n",
    "  model = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                        alpha=reguliser_dampening, verbose=0,\n",
    "                        learning_rate='constant', eta0=learning_rate)\n",
    "\n",
    "  # Train the classifier\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Calculate the training accuracy\n",
    "  training_accuracy = np.sum(model.predict(X_train) == y_train)/len(y_train)\n",
    "\n",
    "  # Store the hyperparameters if they are better than what we have found before\n",
    "  if best_hyperparameters is None or best_hyperparameters[1] < training_accuracy:\n",
    "    best_hyperparameters = (hyperparameters, training_accuracy)\n",
    "  print(\"%.5f\\t\\t%.5f\\t\\t%.1f%%\" % (\n",
    "      hyperparameters['learning_rate'], hyperparameters['reguliser_dampening'], 100*training_accuracy))\n",
    "\n",
    "best_learning_rate = best_hyperparameters[0]['learning_rate']\n",
    "best_reguliser_dampening = best_hyperparameters[0]['reguliser_dampening']\n",
    "print(\"Best parameters: %.5f, %.5f\" %\n",
    "      (best_learning_rate, best_reguliser_dampening))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 14.07, NNZs: 378709, Bias: -0.159716, T: 95805, Avg. loss: 0.772380\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 22.11, NNZs: 379146, Bias: -0.069943, T: 191610, Avg. loss: 0.622904\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 27.98, NNZs: 379474, Bias: 0.016509, T: 287415, Avg. loss: 0.544183\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 32.05, NNZs: 379596, Bias: 0.092215, T: 383220, Avg. loss: 0.498636\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 35.06, NNZs: 379619, Bias: 0.132853, T: 479025, Avg. loss: 0.469870\n",
      "Total training time: 2.06 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 37.34, NNZs: 379638, Bias: 0.191367, T: 574830, Avg. loss: 0.449906\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 39.18, NNZs: 379718, Bias: 0.226143, T: 670635, Avg. loss: 0.435065\n",
      "Total training time: 2.74 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 40.66, NNZs: 379739, Bias: 0.256914, T: 766440, Avg. loss: 0.423601\n",
      "Total training time: 3.11 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 41.90, NNZs: 379833, Bias: 0.282703, T: 862245, Avg. loss: 0.414544\n",
      "Total training time: 3.45 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 42.92, NNZs: 379854, Bias: 0.297942, T: 958050, Avg. loss: 0.407261\n",
      "Total training time: 3.89 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 43.74, NNZs: 379870, Bias: 0.331839, T: 1053855, Avg. loss: 0.401402\n",
      "Total training time: 4.29 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 44.46, NNZs: 379875, Bias: 0.344538, T: 1149660, Avg. loss: 0.396637\n",
      "Total training time: 4.68 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 45.08, NNZs: 379878, Bias: 0.362219, T: 1245465, Avg. loss: 0.392598\n",
      "Total training time: 5.04 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 45.61, NNZs: 379880, Bias: 0.369741, T: 1341270, Avg. loss: 0.389216\n",
      "Total training time: 5.41 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 46.05, NNZs: 379882, Bias: 0.388985, T: 1437075, Avg. loss: 0.386349\n",
      "Total training time: 5.79 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 46.45, NNZs: 379885, Bias: 0.390255, T: 1532880, Avg. loss: 0.383857\n",
      "Total training time: 6.14 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 46.79, NNZs: 379885, Bias: 0.401489, T: 1628685, Avg. loss: 0.381743\n",
      "Total training time: 6.53 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 47.09, NNZs: 379890, Bias: 0.410964, T: 1724490, Avg. loss: 0.379976\n",
      "Total training time: 6.87 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 47.36, NNZs: 379890, Bias: 0.413895, T: 1820295, Avg. loss: 0.378331\n",
      "Total training time: 7.20 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 47.59, NNZs: 379891, Bias: 0.420733, T: 1916100, Avg. loss: 0.376989\n",
      "Total training time: 7.59 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 47.81, NNZs: 379891, Bias: 0.422101, T: 2011905, Avg. loss: 0.375738\n",
      "Total training time: 7.97 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 48.00, NNZs: 379902, Bias: 0.415556, T: 2107710, Avg. loss: 0.374663\n",
      "Total training time: 8.31 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 48.15, NNZs: 379902, Bias: 0.429232, T: 2203515, Avg. loss: 0.373766\n",
      "Total training time: 8.74 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 48.30, NNZs: 379903, Bias: 0.435386, T: 2299320, Avg. loss: 0.372895\n",
      "Total training time: 9.14 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 48.43, NNZs: 379906, Bias: 0.434702, T: 2395125, Avg. loss: 0.372135\n",
      "Total training time: 9.52 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 48.55, NNZs: 379906, Bias: 0.442322, T: 2490930, Avg. loss: 0.371463\n",
      "Total training time: 9.95 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 48.65, NNZs: 379906, Bias: 0.443201, T: 2586735, Avg. loss: 0.370921\n",
      "Total training time: 10.32 seconds.\n",
      "Convergence after 27 epochs took 10.32 seconds\n",
      "Test set accuracy 87.0%\n"
     ]
    }
   ],
   "source": [
    "# Set up the classifier\n",
    "model = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                      alpha=best_reguliser_dampening, verbose=1,\n",
    "                      learning_rate='constant', eta0=best_learning_rate)\n",
    "\n",
    "# Train on all the non-test data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Run prediction on the test set\n",
    "test_accuracy = np.sum(model.predict(X_test) == y_test)/len(y_test)\n",
    "\n",
    "print(\"Test set accuracy %.1f%%\" % (100*test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset\n",
    "with open(\"data_json/SubtaskA/subtaskA_dev_monolingual.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Parse each line as a separate JSON object\n",
    "data = []\n",
    "for line in lines:\n",
    "    obj = json.loads(line)\n",
    "    data.append(obj)\n",
    "\n",
    "# Convert the list of JSON objects to a pandas DataFrame\n",
    "dev_df = pd.DataFrame(data)\n",
    "\n",
    "# Vectorize the text data using TfidfVectorizer\n",
    "X_dev = vectorizer.transform(dev_df['text'])\n",
    "\n",
    "# Predict the labels for the new dataset\n",
    "new_predictions = clf.predict(X_dev)\n",
    "\n",
    "# Convert the predicted probabilities to binary labels\n",
    "new_labels = [1 if p >= 0.5 else 0 for p in new_predictions]\n",
    "\n",
    "# Store the predictions in a separate jsonl file\n",
    "predictions = list(zip(dev_df['id'], new_labels))\n",
    "predictions_df = pd.DataFrame(predictions, columns=['id', 'label'])\n",
    "predictions_df.to_json('SGD_outputs/dev_predictions.jsonl',\n",
    "                       lines=True, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.81      0.63      2500\n",
      "           1       0.57      0.26      0.35      2500\n",
      "\n",
      "    accuracy                           0.53      5000\n",
      "   macro avg       0.55      0.53      0.49      5000\n",
      "weighted avg       0.55      0.53      0.49      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report2_df = pd.DataFrame(\n",
    "    classification_report(dev_df['label'], new_labels, output_dict=True)).transpose()\n",
    "classification_report2_df.to_csv(\n",
    "    'SGD_outputs/classification_report2.csv', index=False)\n",
    "\n",
    "print(classification_report(dev_df['label'], new_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
