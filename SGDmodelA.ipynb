{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import learning_curve, ParameterSampler, train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and preprocessing\n",
    "\n",
    "# Load the dataset\n",
    "with open('data_json/SubtaskA/subtaskA_train_monolingual.jsonl', 'r') as f:\n",
    "    df = pd.read_json(f, lines=True, orient='records')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zuba1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train the SGDClassifier model\n",
    "clf = SGDClassifier(loss='log', penalty='l2', alpha=1e-3,\n",
    "                    random_state=42, verbose=0, max_iter=15, tol=None)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Predict the probabilities of the test set labels\n",
    "y_prob = clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_df = pd.DataFrame(\n",
    "    classification_report(y_test, y_pred, output_dict=True)).transpose()\n",
    "classification_report_df.to_csv(\n",
    "    'SGD_outputs/classification_report.csv', index=False)\n",
    "\n",
    "confusion_df = pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "confusion_df.to_csv('SGD_outputs/confusion_matrix.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from y_test and y_prob\n",
    "roc = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'prob_0': [prob[0] for prob in y_prob],\n",
    "    'prob_1': [prob[1] for prob in y_prob]\n",
    "})\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "roc.to_csv(\"SGD_outputs/ROC.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients of the trained model\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coef = clf.coef_[0]\n",
    "\n",
    "# Create a dictionary of feature names and coefficients\n",
    "features_coef = dict(zip(feature_names, coef))\n",
    "\n",
    "# Sort the dictionary by coefficient value\n",
    "sorted_features = sorted(features_coef.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print the weights of the top 30 words to a file\n",
    "with open(\"SGD_outputs/top_bottom_words.csv\", \"w\") as f:\n",
    "    for word, weight in sorted_features[-30:]:\n",
    "        f.write(f\"{word},{weight}\\n\")\n",
    "\n",
    "    # Print the weights of the bottom 30 words to the same file\n",
    "    for word, weight in sorted_features[:30]:\n",
    "        f.write(f\"{word},{weight}\\n\")\n",
    "\n",
    "# Store all of the weights in a separate csv file\n",
    "weights_df = pd.DataFrame(sorted_features, columns=['word', 'weight'])\n",
    "weights_df.to_csv('SGD_outputs/weights.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "cv = 5\n",
    "\n",
    "# Calculate the learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    clf, X_train, y_train, train_sizes=train_sizes, cv=cv, verbose=0)\n",
    "\n",
    "# Create a pandas DataFrame with the learning curve coordinates\n",
    "df_learning_curve = pd.DataFrame({\n",
    "    'train_sizes': train_sizes,\n",
    "    'train_scores_mean': np.mean(train_scores, axis=1),\n",
    "    'test_scores_mean': np.mean(test_scores, axis=1),\n",
    "    'train_scores_std': np.std(train_scores, axis=1),\n",
    "    'test_scores_std': np.std(test_scores, axis=1)\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a csv file\n",
    "df_learning_curve.to_csv('SGD_outputs/learning_curve.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:\tReg.dampening:\tTraining set accuracy:\n",
      "0.00099\t\t0.00311\t\t59.5%\n",
      "0.03071\t\t0.00099\t\t67.0%\n",
      "0.00311\t\t0.03071\t\t53.1%\n",
      "0.00977\t\t0.30353\t\t53.1%\n",
      "0.00031\t\t3.00000\t\t53.1%\n",
      "3.00000\t\t0.00977\t\t53.1%\n",
      "0.30353\t\t0.30353\t\t53.1%\n",
      "0.00977\t\t0.09655\t\t53.1%\n",
      "0.30353\t\t0.00977\t\t56.5%\n",
      "3.00000\t\t0.09655\t\t53.1%\n",
      "3.00000\t\t0.95425\t\t53.1%\n",
      "0.00977\t\t0.00099\t\t68.1%\n",
      "0.09655\t\t0.00031\t\t83.8%\n",
      "0.00010\t\t0.00977\t\t59.3%\n",
      "3.00000\t\t3.00000\t\t53.1%\n",
      "Best parameters: 0.09655, 0.00031\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "parameter_distribution = {'learning_rate': np.exp(np.linspace(np.log(0.0001), np.log(3), 10)),\n",
    "                          'reguliser_dampening': np.exp(np.linspace(np.log(0.0001), np.log(3), 10))}\n",
    "\n",
    "# Placeholder to make future comparissons easier\n",
    "best_hyperparameters = None\n",
    "print(\"Learning rate:\\tReg.dampening:\\tTraining set accuracy:\")\n",
    "\n",
    "for hyperparameters in ParameterSampler(parameter_distribution, n_iter=15):\n",
    "  # Set up the classifier\n",
    "  reguliser_dampening = hyperparameters['reguliser_dampening']\n",
    "  learning_rate = hyperparameters['learning_rate']\n",
    "  model = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                        alpha=reguliser_dampening, verbose=0,\n",
    "                        learning_rate='constant', eta0=learning_rate)\n",
    "\n",
    "  # Train the classifier\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Calculate the training accuracy\n",
    "  training_accuracy = np.sum(model.predict(X_train) == y_train)/len(y_train)\n",
    "\n",
    "  # Store the hyperparameters if they are better than what we have found before\n",
    "  if best_hyperparameters is None or best_hyperparameters[1] < training_accuracy:\n",
    "    best_hyperparameters = (hyperparameters, training_accuracy)\n",
    "  print(\"%.5f\\t\\t%.5f\\t\\t%.1f%%\" % (\n",
    "      hyperparameters['learning_rate'], hyperparameters['reguliser_dampening'], 100*training_accuracy))\n",
    "\n",
    "best_learning_rate = best_hyperparameters[0]['learning_rate']\n",
    "best_reguliser_dampening = best_hyperparameters[0]['reguliser_dampening']\n",
    "print(\"Best parameters: %.5f, %.5f\" %\n",
    "      (best_learning_rate, best_reguliser_dampening))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 29.21, NNZs: 352566, Bias: 0.115859, T: 95805, Avg. loss: 0.583999\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 29.47, NNZs: 357804, Bias: 0.150616, T: 191610, Avg. loss: 0.520653\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 29.75, NNZs: 359861, Bias: 0.146754, T: 287415, Avg. loss: 0.519226\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 29.79, NNZs: 361334, Bias: 0.109100, T: 383220, Avg. loss: 0.519277\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 29.63, NNZs: 362165, Bias: 0.147720, T: 479025, Avg. loss: 0.519213\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 29.62, NNZs: 362913, Bias: 0.137099, T: 574830, Avg. loss: 0.519251\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 29.53, NNZs: 363489, Bias: 0.142892, T: 670635, Avg. loss: 0.519175\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 29.60, NNZs: 364047, Bias: 0.157375, T: 766440, Avg. loss: 0.519035\n",
      "Total training time: 1.69 seconds.\n",
      "Convergence after 8 epochs took 1.69 seconds\n",
      "Test set accuracy 82.4%\n"
     ]
    }
   ],
   "source": [
    "# Set up the classifier\n",
    "model = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                      alpha=best_reguliser_dampening, verbose=1,\n",
    "                      learning_rate='constant', eta0=best_learning_rate)\n",
    "\n",
    "# Train on all the non-test data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Run prediction on the test set\n",
    "test_accuracy = np.sum(model.predict(X_test) == y_test)/len(y_test)\n",
    "\n",
    "print(\"Test set accuracy %.1f%%\" % (100*test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Load the new dataset\n",
    "with open(\"data_json/SubtaskA/subtaskA_dev_monolingual.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Parse each line as a separate JSON object\n",
    "data = []\n",
    "for line in lines:\n",
    "    obj = json.loads(line)\n",
    "    data.append(obj)\n",
    "\n",
    "# Convert the list of JSON objects to a pandas DataFrame\n",
    "dev_df = pd.DataFrame(data)\n",
    "\n",
    "# Vectorize the text data using TfidfVectorizer\n",
    "X_dev = vectorizer.transform(dev_df['text'])\n",
    "\n",
    "# Predict the labels for the new dataset\n",
    "new_predictions = clf.predict(X_dev)\n",
    "\n",
    "# Convert the predicted probabilities to binary labels\n",
    "new_labels = [1 if p >= 0.5 else 0 for p in new_predictions]\n",
    "\n",
    "# Store the predictions in a separate jsonl file\n",
    "predictions = list(zip(dev_df['id'], new_labels))\n",
    "predictions_df = pd.DataFrame(predictions, columns=['id', 'label'])\n",
    "predictions_df.to_json('SGD_outputs/dev_predictions.jsonl',\n",
    "                       lines=True, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report2_df = pd.DataFrame(\n",
    "    classification_report(dev_df['label'], new_labels, output_dict=True)).transpose()\n",
    "classification_report2_df.to_csv(\n",
    "    'SGD_outputs/classification_report2.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
