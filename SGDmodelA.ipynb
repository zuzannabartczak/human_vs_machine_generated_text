{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data preparation and preprocessing\n",
    "\n",
    "# Load the dataset\n",
    "with open('data_json/SubtaskA/subtaskA_train_monolingual.jsonl', 'r') as f:\n",
    "    df = pd.read_json(f, lines=True, orient='records')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SGDClassifier model\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3,\n",
    "                    random_state=42, verbose=1, max_iter=10, tol=None)\n",
    "\n",
    "with open(\"SGD_outputs/random_training.csv\", \"w\") as f:\n",
    "    # Redirect stdout to file\n",
    "    sys.stdout = f\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Reset stdout\n",
    "    sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the coefficients of the trained model\n",
    "coef = clf.coef_[0]\n",
    "\n",
    "# Create a dictionary of feature names and coefficients\n",
    "features_coef = dict(zip(feature_names, coef))\n",
    "\n",
    "# Sort the dictionary by coefficient value\n",
    "sorted_features = sorted(features_coef.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print the weights of the top 30 words to a file\n",
    "with open(\"SGD_outputs/top_bottom_words.csv\", \"w\") as f:\n",
    "    for word, weight in sorted_features[-30:]:\n",
    "        f.write(f\"{word},{weight}\\n\")\n",
    "\n",
    "    # Print the weights of the bottom 30 words to the same file\n",
    "    for word, weight in sorted_features[:30]:\n",
    "        f.write(f\"{word},{weight}\\n\")\n",
    "\n",
    "# Store all of the weights in a separate csv file\n",
    "weights_df = pd.DataFrame(sorted_features, columns=['word', 'weight'])\n",
    "weights_df.to_csv('SGD_outputs/weights.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "cv = 5\n",
    "\n",
    "# Calculate the learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    clf, X_train, y_train, train_sizes=train_sizes, cv=cv)\n",
    "\n",
    "# Create a pandas DataFrame with the learning curve coordinates\n",
    "df_learning_curve = pd.DataFrame({\n",
    "    'train_sizes': train_sizes,\n",
    "    'train_scores_mean': np.mean(train_scores, axis=1),\n",
    "    'test_scores_mean': np.mean(test_scores, axis=1),\n",
    "    'train_scores_std': np.std(train_scores, axis=1),\n",
    "    'test_scores_std': np.std(test_scores, axis=1)\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a csv file\n",
    "df_learning_curve.to_csv('SGD_outputs/learning_curve.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "parameter_distribution = {'learning_rate': np.exp(np.linspace(np.log(0.0001), np.log(3), 10)),\n",
    "                          'reguliser_dampening': np.exp(np.linspace(np.log(0.0001), np.log(3), 10))}\n",
    "\n",
    "# Placeholder to make future comparissons easier\n",
    "best_hyperparameters = None\n",
    "print(\"Learning rate:\\tReg.dampening:\\tTraining set accuracy:\")\n",
    "\n",
    "for hyperparameters in ParameterSampler(parameter_distribution, n_iter=10):\n",
    "  # Set up the classifier\n",
    "  reguliser_dampening = hyperparameters['reguliser_dampening']\n",
    "  learning_rate = hyperparameters['learning_rate']\n",
    "  model = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                        alpha=reguliser_dampening, verbose=0,\n",
    "                        learning_rate='constant', eta0=learning_rate)\n",
    "\n",
    "  # Train the classifier\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Calculate the training accuracy\n",
    "  training_accuracy = np.sum(model.predict(X_train) == y_train)/len(y_train)\n",
    "\n",
    "  # Store the hyperparameters if they are better than what we have found before\n",
    "  if best_hyperparameters is None or best_hyperparameters[1] < training_accuracy:\n",
    "    best_hyperparameters = (hyperparameters, training_accuracy)\n",
    "  print(\"%.5f\\t\\t%.5f\\t\\t%.1f%%\" % (\n",
    "      hyperparameters['learning_rate'], hyperparameters['reguliser_dampening'], 100*training_accuracy))\n",
    "\n",
    "best_learning_rate = best_hyperparameters[0]['learning_rate']\n",
    "best_reguliser_dampening = best_hyperparameters[0]['reguliser_dampening']\n",
    "print(\"Best parameters: %.5f, %.5f\" %\n",
    "      (best_learning_rate, best_reguliser_dampening))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the classifier\n",
    "model = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                      alpha=best_reguliser_dampening, verbose=1,\n",
    "                      learning_rate='constant', eta0=best_learning_rate)\n",
    "\n",
    "# Train on all the non-test data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Run prediction on the test set\n",
    "test_accuracy = np.sum(model.predict(X_test) == y_test)/len(y_test)\n",
    "\n",
    "print(\"Test set accuracy %.1f%%\" % (100*test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset\n",
    "with open(\"data_json/SubtaskA/subtaskA_dev_monolingual.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Parse each line as a separate JSON object\n",
    "data = []\n",
    "for line in lines:\n",
    "    obj = json.loads(line)\n",
    "    data.append(obj)\n",
    "\n",
    "# Convert the list of JSON objects to a pandas DataFrame\n",
    "new_df = pd.DataFrame(data)\n",
    "\n",
    "# Vectorize the text data using TfidfVectorizer\n",
    "X_dev = vectorizer.transform(new_df['text'])\n",
    "# Predict the labels for the new dataset\n",
    "new_predictions = clf.predict(X_dev)\n",
    "\n",
    "# Convert the predicted probabilities to binary labels\n",
    "new_labels = [1 if p >= 0.5 else 0 for p in new_predictions]\n",
    "\n",
    "# Store the predictions in a separate jsonl file\n",
    "predictions = list(zip(new_df['id'], new_labels))\n",
    "predictions_df = pd.DataFrame(predictions, columns=['id', 'label'])\n",
    "predictions_df.to_json('SGD_outputs/dev_predictions.jsonl',\n",
    "                       lines=True, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred2 = clf.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "report2 = classification_report(y_test, y_pred2)\n",
    "print(report2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
